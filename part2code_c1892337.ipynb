{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"part2code_c1892337.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ryo7VbVhcFJm","colab_type":"text"},"source":["### Part 2"]},{"cell_type":"markdown","metadata":{"id":"0V9PJ7HwcQzx","colab_type":"text"},"source":["Import packages"]},{"cell_type":"code","metadata":{"id":"WmYlRMP5cEow","colab_type":"code","outputId":"91f0e32f-5a42-4298-9b78-e509f622e109","colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# Common imports\n","import numpy as np\n","import os\n","import pandas as pd \n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.classify import SklearnClassifier\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","import re\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import operator\n","\n","\n","from textblob import TextBlob"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9k7WNv4vce_J","colab_type":"code","colab":{}},"source":["## Import Data\n","\n","#Import data from drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","path= '/content/drive/My Drive/CMT307 Applied Machine Learning/Coursework/datasets_coursework1/IMDb/train/imdb_train_pos.txt'\n","path1= '/content/drive/My Drive/CMT307 Applied Machine Learning/Coursework/datasets_coursework1/IMDb/train/imdb_train_neg.txt'\n","path2= '/content/drive/My Drive/CMT307 Applied Machine Learning/Coursework/datasets_coursework1/IMDb/test/imdb_test_pos.txt'\n","path3= '/content/drive/My Drive/CMT307 Applied Machine Learning/Coursework/datasets_coursework1/IMDb/test/imdb_test_neg.txt'\n","path4= '/content/drive/My Drive/CMT307 Applied Machine Learning/Coursework/datasets_coursework1/IMDb/dev/imdb_dev_pos.txt'\n","path5= '/content/drive/My Drive/CMT307 Applied Machine Learning/Coursework/datasets_coursework1/IMDb/dev/imdb_dev_neg.txt'\n","\n","\n","train_pos = open(path).readlines()\n","train_neg = open(path1).readlines() \n","test_pos = open(path2).readlines()\n","test_neg = open(path3).readlines()   \n","dev_pos = open(path4).readlines()\n","dev_neg = open(path5).readlines()   \n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UhqUCmfrdtVP","colab_type":"code","colab":{}},"source":["# Convert to pandas DF\n","\n","df_train_pos = pd.DataFrame(train_pos)\n","df_train_neg = pd.DataFrame(train_neg)\n","df_test_pos = pd.DataFrame(test_pos)\n","df_test_neg = pd.DataFrame(test_neg)\n","df_dev_pos = pd.DataFrame(dev_pos)\n","df_dev_neg = pd.DataFrame(dev_neg)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqLwJMXZgvnA","colab_type":"code","colab":{}},"source":["df_train_pos['sentiment'] = 1\n","df_train_neg['sentiment'] = 0\n","df_test_pos['sentiment'] = 1\n","df_test_neg['sentiment'] = 0\n","df_dev_pos['sentiment'] = 1\n","df_dev_neg['sentiment'] = 0\n","\n","# rename columns\n","df_train_pos.columns = ['review', 'sentiment']\n","df_train_neg.columns = ['review', 'sentiment']\n","df_test_pos.columns = ['review', 'sentiment']\n","df_test_neg.columns = ['review', 'sentiment']\n","df_dev_pos.columns = ['review', 'sentiment']\n","df_dev_neg.columns = ['review', 'sentiment']\n","\n","print(len(df_train_neg))\n","print(len(df_train_pos))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OSwMMM4OJcWx","colab_type":"code","colab":{}},"source":["# Concatenate positive and negative dataframes\n","df_train = pd.concat([df_train_pos, df_train_neg], ignore_index = True)\n","print(len(df_train))\n","df_test = pd.concat([df_test_pos, df_test_neg], ignore_index = True)\n","print(len(df_test))\n","df_val = pd.concat([df_dev_pos, df_dev_neg], ignore_index = True)\n","print(len(df_val))\n","\n","df_train.columns = ['review', 'sentiment']\n","df_test.columns = ['review', 'sentiment']\n","df_val.columns = ['review', 'sentiment']\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3L_4nigFjVTq","colab_type":"text"},"source":["## Functions"]},{"cell_type":"code","metadata":{"id":"WSl2YmDm_lAj","colab_type":"code","colab":{}},"source":["# First, we get the stopwords list from nltk\n","stopwords=set(nltk.corpus.stopwords.words('english'))\n","# We can add more words to the stopword list, like punctuation marks\n","stopwords.add(\".\")\n","stopwords.add(\",\")\n","stopwords.add(\"-\")\n","stopwords.add(\"``\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OI_dTpIS3mO1","colab_type":"code","colab":{}},"source":["wn = WordNetLemmatizer()\n","\n","\n","def StopWords(token):\n","    return  token not in stopwords and token not in list(string.punctuation)  and len(token)>2 \n","\n","\n","def clean_text(text):\n","  clean_text = []\n","  clean_text2 = []\n","  text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n","                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', text) # Removing urls\n","  text = re.sub(r'[^\\w\\s]', '',text)\n","  text = re.sub(\"'\", \"\",text)\n","  text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)    \n","  clean_text = [ wn.lemmatize(word, pos=\"v\") for word in word_tokenize(text.lower()) if StopWords(word)]\n","  clean_text2 = [word for word in clean_text if StopWords(word)]\n","  return \" \".join(clean_text2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PUj7SN3QUk2J","colab_type":"code","colab":{}},"source":["import textblob\n","\n","## Add length of review as a feature\n","def len_text(review):\n","  if len(review.split())>0:\n","    return len(set(clean_text(review).split())) /len(review.split())\n","  else:\n","    return 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_xfxj6JZgaP","colab_type":"code","colab":{}},"source":["# returns whether the text is objective or subjective\n","def subjectivity_text(review):\n","  return TextBlob(review).sentiment[1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKfHBcZyjmAD","colab_type":"code","colab":{}},"source":["# returns polarity\n","def polarity_text(review):\n","  return TextBlob(review).sentiment[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UM-ovztnjwGx","colab_type":"code","colab":{}},"source":["## Add full length of review as a feature\n","def full_len_text(review):\n","  if len(review.split())>0:\n","    return len(set(clean_text(review).split()))\n","  else:\n","    return 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYBzfExbBQpb","colab_type":"code","colab":{}},"source":["lemmatizer = nltk.stem.WordNetLemmatizer()\n","\n","# returns sentence as tokens\n","def get_list_tokens(string):\n","  sentence_split=nltk.tokenize.sent_tokenize(string)\n","  list_tokens=[]\n","  for sentence in sentence_split:\n","    sentence = clean_text(sentence)\n","    \n","    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n","    for token in list_tokens_sentence:\n","      list_tokens.append(lemmatizer.lemmatize(token).lower())\n","  return list_tokens"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FsD6bC_BYAJ","colab_type":"code","colab":{}},"source":["lemmatizer = nltk.stem.WordNetLemmatizer()\n","\n","def get_vocabulary(data, num_features): # Function to retrieve vocabulary\n","\n","  dict_word_frequency={}\n","  for sentence in data:\n","    sentence_tokens=get_list_tokens(sentence)\n","    for word1 in sentence_tokens:\n","      word = lemmatizer.lemmatize(word1)\n","      if word in stopwords: continue\n","      if word not in dict_word_frequency: dict_word_frequency[word]=1\n","      else: dict_word_frequency[word]+=1\n","        \n","  # Now we create a sorted frequency list with the top N words, using the function \"sorted\". Let's see the 15 most frequent words\n","  sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:num_features]\n","  i=0\n","  for word,frequency in sorted_list:\n","    i+=1\n","    \n","    \n","  # Finally, we create our vocabulary based on the sorted frequency list \n","  vocabulary=[]\n","  for word,frequency in sorted_list:\n","    vocabulary.append(word)\n","  return vocabulary"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aDDsktgqBhb1","colab_type":"code","colab":{}},"source":["\n","def get_vector_text(list_vocab,string):\n","  vector_text=np.zeros(len(list_vocab))\n","  list_tokens_string=get_list_tokens(string)\n","  for i, word in enumerate(list_vocab):\n","    if word in list_tokens_string:\n","      vector_text[i]=list_tokens_string.count(lemmatizer.lemmatize(word))\n","  return vector_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4yklfWpsRQcm","colab_type":"code","colab":{}},"source":["def get_negative_vector_text(string):\n","  vector_text=np.zeros(len(neg_vocabulary))\n","  list_tokens_string=get_list_tokens(string)\n","  for i, word in enumerate(neg_vocabulary):\n","    if word in list_tokens_string:\n","      vector_text[i]=list_tokens_string.count(lemmatizer.lemmatize(word))\n","  return vector_text\n","\n","def get_positive_vector_text(string):\n","  vector_text=np.zeros(len(pos_vocabulary))\n","  list_tokens_string=get_list_tokens(string)\n","  for i, word in enumerate(pos_vocabulary):\n","    if word in list_tokens_string:\n","      vector_text[i]=list_tokens_string.count(lemmatizer.lemmatize(word))\n","  return vector_text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PpAWKU3kR-Ad","colab_type":"text"},"source":["# Data Pre Processing"]},{"cell_type":"code","metadata":{"id":"YEaF4W5ij1sa","colab_type":"code","colab":{}},"source":["df_train['polarity'] = df_train['review'].apply(polarity_text)\n","df_test['polarity'] = df_test['review'].apply(polarity_text)\n","df_val['polarity'] = df_val['review'].apply(polarity_text)\n","\n","df_train.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTyHkpuJw6Jg","colab_type":"code","colab":{}},"source":["df_train['subjectivity'] = df_train['review'].apply(subjectivity_text)\n","df_test['subjectivity'] = df_test['review'].apply(subjectivity_text)\n","df_val['subjectivity'] = df_val['review'].apply(subjectivity_text)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CmxIUys_yHoU","colab_type":"code","colab":{}},"source":["df_train.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_eT0QCJVsS_E","colab_type":"code","colab":{}},"source":["df_train['length'] = df_train['review'].apply(len_text)\n","df_test['length'] = df_test['review'].apply(len_text)\n","df_val['length'] = df_val['review'].apply(len_text)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tbxUE7ta4rpp","colab_type":"code","colab":{}},"source":["df_train.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RobUyekdDho","colab_type":"code","colab":{}},"source":["# Full_Length of the cleaned data\n","df_train['full_length'] = df_train['review'].apply(full_len_text)\n","df_test['full_length'] = df_test['review'].apply(full_len_text)\n","df_val['full_length'] = df_val['review'].apply(full_len_text)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eCXon9qMFAjL","colab_type":"text"},"source":["Check correlation of features"]},{"cell_type":"markdown","metadata":{"id":"Lp1Sa-VaoIgB","colab_type":"text"},"source":["Make the Custom class for feature union Transformer of sklearn"]},{"cell_type":"code","metadata":{"id":"zjIsNyJ9oHnK","colab_type":"code","colab":{}},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import FeatureUnion\n","from sklearn.feature_extraction import DictVectorizer\n","class ItemSelector(BaseEstimator, TransformerMixin):\n","    def __init__(self, key):\n","        self.key = key\n","\n","    def fit(self, x, y=None):\n","        return self\n","\n","    def transform(self, data_dict):\n","        return data_dict[self.key]\n","\n","\n","class TextStatistics(BaseEstimator, TransformerMixin):\n","    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n","\n","    def fit(self, x, y=None):\n","        return self\n","\n","    def transform(self, data):\n","        return [{'pos':  row['polarity'], 'sub': row['subjectivity'],  'length': row['length'], 'full_length': row['full_length']} for _, row in data.iterrows()]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6HNUovr9jFAW","colab_type":"text"},"source":["Create a pipeline"]},{"cell_type":"code","metadata":{"id":"bf00bMoGRgOy","colab_type":"code","colab":{}},"source":[" from sklearn.pipeline import Pipeline\n","\n","pipeline = Pipeline([\n","    ('union', FeatureUnion(\n","        transformer_list=[\n","\n","            # Pipeline for pulling features from the text\n","            ('review', Pipeline([\n","                ('selector', ItemSelector(key='review')),\n","                ('tfidf', TfidfVectorizer( min_df =3, max_df=0.3, max_features=1000, \n","                    strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n","                    ngram_range=(1, 10), use_idf=1,smooth_idf=1,sublinear_tf=1,\n","                    stop_words = None, preprocessor=clean_text)),\n","            ])),\n","\n","            # Pipeline for pulling features\n","            ('statistics', Pipeline([\n","                ('selector', ItemSelector(key=['polarity', 'subjectivity', 'length','full_length'])),\n","                ('statistics', TextStatistics()),  # returns a list of dicts\n","                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n","            ])),\n","\n","        ],\n","\n","        # weight components in FeatureUnion\n","        transformer_weights={\n","            'review': 1,\n","            'statistics': 1,\n","        },\n","    ))\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2gBdLSvoSNg","colab_type":"text"},"source":["Build the pipeline"]},{"cell_type":"code","metadata":{"id":"RaAs4siVoTYK","colab_type":"code","colab":{}},"source":["x_train = df_train[['review', 'polarity', 'subjectivity','length', 'full_length']]\n","y_train =df_train['sentiment']\n","\n","x_test = df_test[['review', 'polarity', 'subjectivity','length', 'full_length']]\n","y_test =df_test['sentiment']\n","\n","x_val = df_val[['review', 'polarity', 'subjectivity','length', 'full_length']]\n","y_val =df_val['sentiment']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgUC5piwom3H","colab_type":"code","colab":{}},"source":["pipeline.fit(x_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pojxzebiu6cm","colab_type":"code","colab":{}},"source":["# Create our set of Vocabulary   \n","vocabulary=get_vocabulary(x_train.review , 500)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MFmueFW3rtFM","colab_type":"code","colab":{}},"source":["train_vector1 = pipeline.transform(x_train)\n","test_vector1 = pipeline.transform(x_test)\n","val_vector1 = pipeline.transform(x_val)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IPrHdrKwFmG","colab_type":"code","colab":{}},"source":["train_vector2=[]\n","test_vector2=[]\n","val_vector2=[]\n","\n","for row in x_train.review:\n","    vector_instance=get_vector_text(vocabulary,row)\n","    train_vector2.append(vector_instance)\n","\n","for row in x_test.review:\n","    vector_instance=get_vector_text(vocabulary,row)\n","    test_vector2.append(vector_instance)\n","\n","for row in x_val.review:\n","    vector_instance=get_vector_text(vocabulary,row)\n","    val_vector2.append(vector_instance)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"62SQwHc5Ck7m","colab_type":"code","colab":{}},"source":["from scipy.sparse import csr_matrix, hstack\n","train_vector = hstack((csr_matrix(train_vector2),train_vector1))\n","test_vector = hstack((csr_matrix(test_vector2),test_vector1))\n","val_vector = hstack((csr_matrix(val_vector2),val_vector1))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5fvziPnvhZL","colab_type":"text"},"source":["Build the models"]},{"cell_type":"code","metadata":{"id":"Cu1puvSkp-H-","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","\n","# SVM classifier\n","svm_clf=SVC(gamma='auto') # Initialize the SVM model\n","\n","# Logistic regression\n","lr_clf = LogisticRegression()\n","\n","# Decision Tree Classifier\n","dt_clf = DecisionTreeClassifier()\n","\n","# naive bayes\n","nb_clf = GaussianNB()\n","\n","# random forest\n","rf_clf=RandomForestClassifier(n_estimators=100)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hLyNKADfqNzW","colab_type":"code","colab":{}},"source":["import pandas as pd \n","from pandas import DataFrame\n","from sklearn.model_selection import cross_validate\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import precision_score\n","\n","lst = []\n","list_result =[]\n","cols = ['Model', 'Fold','Accuracy']\n","cols1 = ['Model', 'Accuracy', 'Recall', 'Precision', 'F1 - Score']\n","\n","clfs = [svm_clf, lr_clf, dt_clf, rf_clf]\n","cv = 5\n","scoring = {'acc': 'accuracy',\n","           'prec_macro': 'precision_macro',\n","           'rec_micro': 'recall_macro',\n","           'f1_score': 'f1'}\n","\n","for clf in clfs:\n","    scores = cross_validate(clf,train_vector, y_train, cv=cv, scoring=scoring )\n","\n","    lst.append([str(clf), str(scores['test_acc'].mean()) ,str(scores['test_rec_micro'].mean()) ,str(scores['test_prec_macro'].mean()) ,str(scores['test_f1_score'].mean()) ]) \n","    \n","\n","    # Predict validation set\n","    clf.fit(train_vector, y_train )\n","    y_pred = clf.predict(val_vector)\n","    list_result.append((str(clf),accuracy_score(y_val, y_pred),recall_score(y_val, y_pred),precision_score(y_val, y_pred),f1_score(y_val, y_pred),))\n","\n","    print('Model ' + str(clf) + ' Complete' )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OXaJSrJjKM6-","colab_type":"code","colab":{}},"source":["print('Cross validation results')\n","dfResults1 = pd.DataFrame(lst, columns = cols1)\n","dfResults1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HGUKpb6ZYomS","colab_type":"code","colab":{}},"source":["print('Predict Validation set - Scores')\n","dfResultsValPredict = pd.DataFrame(list_result, columns = cols1)\n","dfResultsValPredict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6WTsXxEw5UlU","colab_type":"code","colab":{}},"source":["# Final model validation\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","list_result =[]\n","\n","lr_clf.fit(train_vector, y_train )\n","y_pred = lr_clf.predict(val_vector)\n","list_result.append((\"Logistic Regression\",accuracy_score(y_test, y_pred)))\n","\n","array = confusion_matrix(y_test, y_pred)\n","\n","print(list_result)\n","\n","print(y_pred)\n","\n","\n","\n","   \n","df_cm = pd.DataFrame(array, range(2),\n","                  range(2))\n","#plt.figure(figsize = (10,7))\n","sn.set(font_scale=1.4)#for label size\n","sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})# font size\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9mbKAMGLLmQD","colab_type":"code","colab":{}},"source":["\n","\n"],"execution_count":0,"outputs":[]}]}
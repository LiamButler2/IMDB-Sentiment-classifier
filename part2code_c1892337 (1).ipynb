{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"part2code_c1892337.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ryo7VbVhcFJm","colab_type":"text"},"source":["### Part 2 - Sentiment Classification of IMDB Dataset"]},{"cell_type":"markdown","metadata":{"id":"-Tq5I_Rvw3K6","colab_type":"text"},"source":["Created by Liam Butler"]},{"cell_type":"markdown","metadata":{"id":"ybzwC8VVxbGG","colab_type":"text"},"source":["Uses the IMDb datasets to create a model which is able to predict sentiment of a given review. "]},{"cell_type":"markdown","metadata":{"id":"0V9PJ7HwcQzx","colab_type":"text"},"source":["Import packages"]},{"cell_type":"code","metadata":{"id":"WmYlRMP5cEow","colab_type":"code","outputId":"d645ca6d-00a3-4b5a-884d-fd1c0e8b3ca9","executionInfo":{"status":"ok","timestamp":1578750688380,"user_tz":0,"elapsed":1025,"user":{"displayName":"Liam Butler","photoUrl":"","userId":"09361222935182582455"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# Common imports\n","import numpy as np\n","import os\n","import pandas as pd \n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.classify import SklearnClassifier\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","import re\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import operator\n","import requests\n","\n","from textblob import TextBlob"],"execution_count":32,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9k7WNv4vce_J","colab_type":"code","colab":{}},"source":["## Import Data\n","\n","#Import data from GitHub Repo\n","\n","root='https://raw.githubusercontent.com/LiamButler2/IMDB-Sentiment-classifier/master/datasets_coursework1/IMDb'\n","\n","path= root + '/train/imdb_train_pos.txt'\n","path1= root + '/train/imdb_train_neg.txt'\n","path2= root + '/test/imdb_test_pos.txt'\n","path3= root + '/test/imdb_test_neg.txt'\n","path4= root + '/dev/imdb_dev_pos.txt'\n","path5= root + '/dev/imdb_dev_neg.txt'\n","\n","response = requests.get(path)\n","train_pos = response.text.split(\"\\n\")\n","\n","response = requests.get(path1)\n","train_neg = response.text.split(\"\\n\")\n","\n","response = requests.get(path2)\n","test_pos = response.text.split(\"\\n\")\n","\n","response = requests.get(path3)\n","test_neg = response.text.split(\"\\n\") \n","\n","response = requests.get(path4)\n","dev_pos = response.text.split(\"\\n\")\n","\n","response = requests.get(path5)\n","dev_neg = response.text.split(\"\\n\")   \n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UhqUCmfrdtVP","colab_type":"code","colab":{}},"source":["# Convert to pandas DF\n","\n","df_train_pos = pd.DataFrame(train_pos)\n","df_train_neg = pd.DataFrame(train_neg)\n","df_test_pos = pd.DataFrame(test_pos)\n","df_test_neg = pd.DataFrame(test_neg)\n","df_dev_pos = pd.DataFrame(dev_pos)\n","df_dev_neg = pd.DataFrame(dev_neg)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqLwJMXZgvnA","colab_type":"code","outputId":"02a09864-8b37-474f-ed42-7f83694dadd0","executionInfo":{"status":"ok","timestamp":1578750693654,"user_tz":0,"elapsed":6249,"user":{"displayName":"Liam Butler","photoUrl":"","userId":"09361222935182582455"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Add sentiment\n","df_train_pos['sentiment'] = 1\n","df_train_neg['sentiment'] = 0\n","df_test_pos['sentiment'] = 1\n","df_test_neg['sentiment'] = 0\n","df_dev_pos['sentiment'] = 1\n","df_dev_neg['sentiment'] = 0\n","\n","# Rename columns\n","df_train_pos.columns = ['review', 'sentiment']\n","df_train_neg.columns = ['review', 'sentiment']\n","df_test_pos.columns = ['review', 'sentiment']\n","df_test_neg.columns = ['review', 'sentiment']\n","df_dev_pos.columns = ['review', 'sentiment']\n","df_dev_neg.columns = ['review', 'sentiment']\n","\n","print(len(df_train_neg))\n","print(len(df_train_pos))\n"],"execution_count":35,"outputs":[{"output_type":"stream","text":["7518\n","7484\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OSwMMM4OJcWx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"1fb7be4d-d4a9-4f01-99c4-7a0ae5f04eeb","executionInfo":{"status":"ok","timestamp":1578750693655,"user_tz":0,"elapsed":6216,"user":{"displayName":"Liam Butler","photoUrl":"","userId":"09361222935182582455"}}},"source":["# Concatenate positive and negative dataframes\n","df_train = pd.concat([df_train_pos, df_train_neg], ignore_index = True)\n","print(len(df_train))\n","df_test = pd.concat([df_test_pos, df_test_neg], ignore_index = True)\n","print(len(df_test))\n","df_val = pd.concat([df_dev_pos, df_dev_neg], ignore_index = True)\n","print(len(df_val))\n","\n","df_train.columns = ['review', 'sentiment']\n","df_test.columns = ['review', 'sentiment']\n","df_val.columns = ['review', 'sentiment']\n"],"execution_count":36,"outputs":[{"output_type":"stream","text":["15002\n","5002\n","5002\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3L_4nigFjVTq","colab_type":"text"},"source":["## Functions"]},{"cell_type":"code","metadata":{"id":"WSl2YmDm_lAj","colab_type":"code","colab":{}},"source":["# First, we get the stopwords list from nltk\n","stopwords=set(nltk.corpus.stopwords.words('english'))\n","# We can add more words to the stopword list, like punctuation marks\n","stopwords.add(\".\")\n","stopwords.add(\",\")\n","stopwords.add(\"-\")\n","stopwords.add(\"``\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OI_dTpIS3mO1","colab_type":"code","colab":{}},"source":["wn = WordNetLemmatizer()\n","import string\n","\n","def StopWords(token):\n","    return  token not in stopwords and token not in list(string.punctuation)  and len(token)>2 \n","\n","\n","def clean_text(text):\n","  clean_text = []\n","  clean_text2 = []\n","  text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n","                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', text) # Removing urls\n","  text = re.sub(r'[^\\w\\s]', '',text)\n","  text = re.sub(\"'\", \"\",text)\n","  text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)    \n","  clean_text = [ wn.lemmatize(word, pos=\"v\") for word in word_tokenize(text.lower()) if StopWords(word)]\n","  clean_text2 = [word for word in clean_text if StopWords(word)]\n","  return \" \".join(clean_text2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PUj7SN3QUk2J","colab_type":"code","colab":{}},"source":["import textblob\n","\n","## Add length of review as a feature\n","def len_text(review):\n","  if len(review.split())>0:\n","    return len(set(clean_text(review).split())) /len(review.split())\n","  else:\n","    return 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_xfxj6JZgaP","colab_type":"code","colab":{}},"source":["# returns whether the text is objective or subjective\n","def subjectivity_text(review):\n","  return TextBlob(review).sentiment[1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKfHBcZyjmAD","colab_type":"code","colab":{}},"source":["# returns polarity\n","def polarity_text(review):\n","  return TextBlob(review).sentiment[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UM-ovztnjwGx","colab_type":"code","colab":{}},"source":["## Add full length of review as a feature\n","def full_len_text(review):\n","  if len(review.split())>0:\n","    return len(set(clean_text(review).split()))\n","  else:\n","    return 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYBzfExbBQpb","colab_type":"code","colab":{}},"source":["lemmatizer = nltk.stem.WordNetLemmatizer()\n","\n","# returns sentence as tokens\n","def get_list_tokens(string):\n","  sentence_split=nltk.tokenize.sent_tokenize(string)\n","  list_tokens=[]\n","  for sentence in sentence_split:\n","    sentence = clean_text(sentence)\n","    \n","    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n","    for token in list_tokens_sentence:\n","      list_tokens.append(lemmatizer.lemmatize(token).lower())\n","  return list_tokens"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FsD6bC_BYAJ","colab_type":"code","colab":{}},"source":["lemmatizer = nltk.stem.WordNetLemmatizer()\n","\n","def get_vocabulary(data, num_features): # Function to retrieve vocabulary\n","\n","  dict_word_frequency={}\n","  for sentence in data:\n","    sentence_tokens=get_list_tokens(sentence)\n","    for word1 in sentence_tokens:\n","      word = lemmatizer.lemmatize(word1)\n","      if word in stopwords: continue\n","      if word not in dict_word_frequency: dict_word_frequency[word]=1\n","      else: dict_word_frequency[word]+=1\n","        \n","  # Now we create a sorted frequency list with the top N words, using the function \"sorted\". Let's see the 15 most frequent words\n","  sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:num_features]\n","  i=0\n","  for word,frequency in sorted_list:\n","    i+=1\n","    \n","    \n","  # Finally, we create our vocabulary based on the sorted frequency list \n","  vocabulary=[]\n","  for word,frequency in sorted_list:\n","    vocabulary.append(word)\n","  return vocabulary"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aDDsktgqBhb1","colab_type":"code","colab":{}},"source":["\n","def get_vector_text(list_vocab,string):\n","  vector_text=np.zeros(len(list_vocab))\n","  list_tokens_string=get_list_tokens(string)\n","  for i, word in enumerate(list_vocab):\n","    if word in list_tokens_string:\n","      vector_text[i]=list_tokens_string.count(lemmatizer.lemmatize(word))\n","  return vector_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4yklfWpsRQcm","colab_type":"code","colab":{}},"source":["def get_negative_vector_text(string):\n","  vector_text=np.zeros(len(neg_vocabulary))\n","  list_tokens_string=get_list_tokens(string)\n","  for i, word in enumerate(neg_vocabulary):\n","    if word in list_tokens_string:\n","      vector_text[i]=list_tokens_string.count(lemmatizer.lemmatize(word))\n","  return vector_text\n","\n","def get_positive_vector_text(string):\n","  vector_text=np.zeros(len(pos_vocabulary))\n","  list_tokens_string=get_list_tokens(string)\n","  for i, word in enumerate(pos_vocabulary):\n","    if word in list_tokens_string:\n","      vector_text[i]=list_tokens_string.count(lemmatizer.lemmatize(word))\n","  return vector_text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PpAWKU3kR-Ad","colab_type":"text"},"source":["# Data Pre Processing"]},{"cell_type":"code","metadata":{"id":"YEaF4W5ij1sa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":335},"outputId":"b1669282-81f3-45ca-e0c3-68ae10f47c73","executionInfo":{"status":"ok","timestamp":1578750737854,"user_tz":0,"elapsed":50184,"user":{"displayName":"Liam Butler","photoUrl":"","userId":"09361222935182582455"}}},"source":["# Add Polarity\n","df_train['polarity'] = df_train['review'].apply(polarity_text)\n","df_test['polarity'] = df_test['review'].apply(polarity_text)\n","df_val['polarity'] = df_val['review'].apply(polarity_text)\n","\n","df_train.head(10)"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","      <th>polarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>For fans of Chris Farley, this is probably his...</td>\n","      <td>1</td>\n","      <td>0.181676</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Fantastic, Madonna at her finest, the film is ...</td>\n","      <td>1</td>\n","      <td>0.378125</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>From a perspective that it is possible to make...</td>\n","      <td>1</td>\n","      <td>0.111111</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What is often neglected about Harold Lloyd is ...</td>\n","      <td>1</td>\n","      <td>0.122631</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>You'll either love or hate movies such as this...</td>\n","      <td>1</td>\n","      <td>-0.019647</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Good (not great) little horror film with a hig...</td>\n","      <td>1</td>\n","      <td>0.124583</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>The word Ghilli actually means a small sharp w...</td>\n","      <td>1</td>\n","      <td>0.060473</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>This is one of THE century's best tv-series ev...</td>\n","      <td>1</td>\n","      <td>0.344444</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>It's pretty surprising that this wonderful fil...</td>\n","      <td>1</td>\n","      <td>0.251122</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>A fantastic cinema experience. I really enjoye...</td>\n","      <td>1</td>\n","      <td>0.108333</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review  sentiment  polarity\n","0  For fans of Chris Farley, this is probably his...          1  0.181676\n","1  Fantastic, Madonna at her finest, the film is ...          1  0.378125\n","2  From a perspective that it is possible to make...          1  0.111111\n","3  What is often neglected about Harold Lloyd is ...          1  0.122631\n","4  You'll either love or hate movies such as this...          1 -0.019647\n","5  Good (not great) little horror film with a hig...          1  0.124583\n","6  The word Ghilli actually means a small sharp w...          1  0.060473\n","7  This is one of THE century's best tv-series ev...          1  0.344444\n","8  It's pretty surprising that this wonderful fil...          1  0.251122\n","9  A fantastic cinema experience. I really enjoye...          1  0.108333"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"HTyHkpuJw6Jg","colab_type":"code","colab":{}},"source":["# Add subjectivity\n","df_train['subjectivity'] = df_train['review'].apply(subjectivity_text)\n","df_test['subjectivity'] = df_test['review'].apply(subjectivity_text)\n","df_val['subjectivity'] = df_val['review'].apply(subjectivity_text)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_eT0QCJVsS_E","colab_type":"code","colab":{}},"source":["# relative length\n","df_train['length'] = df_train['review'].apply(len_text)\n","df_test['length'] = df_test['review'].apply(len_text)\n","df_val['length'] = df_val['review'].apply(len_text)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RobUyekdDho","colab_type":"code","colab":{}},"source":["# Full_Length of the cleaned data\n","df_train['full_length'] = df_train['review'].apply(full_len_text)\n","df_test['full_length'] = df_test['review'].apply(full_len_text)\n","df_val['full_length'] = df_val['review'].apply(full_len_text)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tbxUE7ta4rpp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":335},"outputId":"9dd47130-c735-4876-9e2c-c7eeefd05785","executionInfo":{"status":"ok","timestamp":1578750877954,"user_tz":0,"elapsed":190213,"user":{"displayName":"Liam Butler","photoUrl":"","userId":"09361222935182582455"}}},"source":["df_train.head(10)"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","      <th>polarity</th>\n","      <th>subjectivity</th>\n","      <th>length</th>\n","      <th>full_length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>For fans of Chris Farley, this is probably his...</td>\n","      <td>1</td>\n","      <td>0.181676</td>\n","      <td>0.692045</td>\n","      <td>0.605769</td>\n","      <td>63</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Fantastic, Madonna at her finest, the film is ...</td>\n","      <td>1</td>\n","      <td>0.378125</td>\n","      <td>0.712500</td>\n","      <td>0.410714</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>From a perspective that it is possible to make...</td>\n","      <td>1</td>\n","      <td>0.111111</td>\n","      <td>0.408333</td>\n","      <td>0.420290</td>\n","      <td>87</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What is often neglected about Harold Lloyd is ...</td>\n","      <td>1</td>\n","      <td>0.122631</td>\n","      <td>0.490020</td>\n","      <td>0.351425</td>\n","      <td>259</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>You'll either love or hate movies such as this...</td>\n","      <td>1</td>\n","      <td>-0.019647</td>\n","      <td>0.488426</td>\n","      <td>0.508197</td>\n","      <td>62</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Good (not great) little horror film with a hig...</td>\n","      <td>1</td>\n","      <td>0.124583</td>\n","      <td>0.528782</td>\n","      <td>0.538462</td>\n","      <td>77</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>The word Ghilli actually means a small sharp w...</td>\n","      <td>1</td>\n","      <td>0.060473</td>\n","      <td>0.492848</td>\n","      <td>0.419540</td>\n","      <td>73</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>This is one of THE century's best tv-series ev...</td>\n","      <td>1</td>\n","      <td>0.344444</td>\n","      <td>0.361111</td>\n","      <td>0.375000</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>It's pretty surprising that this wonderful fil...</td>\n","      <td>1</td>\n","      <td>0.251122</td>\n","      <td>0.532872</td>\n","      <td>0.405263</td>\n","      <td>154</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>A fantastic cinema experience. I really enjoye...</td>\n","      <td>1</td>\n","      <td>0.108333</td>\n","      <td>0.437698</td>\n","      <td>0.455752</td>\n","      <td>103</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review  ...  full_length\n","0  For fans of Chris Farley, this is probably his...  ...           63\n","1  Fantastic, Madonna at her finest, the film is ...  ...           23\n","2  From a perspective that it is possible to make...  ...           87\n","3  What is often neglected about Harold Lloyd is ...  ...          259\n","4  You'll either love or hate movies such as this...  ...           62\n","5  Good (not great) little horror film with a hig...  ...           77\n","6  The word Ghilli actually means a small sharp w...  ...           73\n","7  This is one of THE century's best tv-series ev...  ...           24\n","8  It's pretty surprising that this wonderful fil...  ...          154\n","9  A fantastic cinema experience. I really enjoye...  ...          103\n","\n","[10 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"Lp1Sa-VaoIgB","colab_type":"text"},"source":["Make the Custom class for feature union Transformer of sklearn"]},{"cell_type":"code","metadata":{"id":"zjIsNyJ9oHnK","colab_type":"code","colab":{}},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import FeatureUnion\n","from sklearn.feature_extraction import DictVectorizer\n","class ItemSelector(BaseEstimator, TransformerMixin):\n","    def __init__(self, key):\n","        self.key = key\n","\n","    def fit(self, x, y=None):\n","        return self\n","\n","    def transform(self, data_dict):\n","        return data_dict[self.key]\n","\n","\n","class TextStatistics(BaseEstimator, TransformerMixin):\n","    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n","\n","    def fit(self, x, y=None):\n","        return self\n","\n","    def transform(self, data):\n","        return [{'pos':  row['polarity'], 'sub': row['subjectivity'],  'length': row['length'], 'full_length': row['full_length']} for _, row in data.iterrows()]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6HNUovr9jFAW","colab_type":"text"},"source":["Create a pipeline"]},{"cell_type":"code","metadata":{"id":"bf00bMoGRgOy","colab_type":"code","colab":{}},"source":[" from sklearn.pipeline import Pipeline\n","\n","pipeline = Pipeline([\n","    ('union', FeatureUnion(\n","        transformer_list=[\n","\n","            # Pipeline for pulling features from the text\n","            ('review', Pipeline([\n","                ('selector', ItemSelector(key='review')),\n","                ('tfidf', TfidfVectorizer( min_df =3, max_df=0.3, max_features=1000, \n","                    strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n","                    ngram_range=(1, 10), use_idf=1,smooth_idf=1,sublinear_tf=1,\n","                    stop_words = None, preprocessor=clean_text)),\n","            ])),\n","\n","            # Pipeline for pulling features\n","            ('statistics', Pipeline([\n","                ('selector', ItemSelector(key=['polarity', 'subjectivity', 'length','full_length'])),\n","                ('statistics', TextStatistics()),  # returns a list of dicts\n","                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n","            ])),\n","\n","        ],\n","\n","        # weight components in FeatureUnion\n","        transformer_weights={\n","            'review': 1,\n","            'statistics': 1,\n","        },\n","    ))\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2gBdLSvoSNg","colab_type":"text"},"source":["Build the pipeline"]},{"cell_type":"code","metadata":{"id":"RaAs4siVoTYK","colab_type":"code","colab":{}},"source":["x_train = df_train[['review', 'polarity', 'subjectivity','length', 'full_length']]\n","y_train =df_train['sentiment']\n","\n","x_test = df_test[['review', 'polarity', 'subjectivity','length', 'full_length']]\n","y_test =df_test['sentiment']\n","\n","x_val = df_val[['review', 'polarity', 'subjectivity','length', 'full_length']]\n","y_val =df_val['sentiment']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgUC5piwom3H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":700},"outputId":"49c9a7f4-9864-41e8-987b-30104f8f8135","executionInfo":{"status":"ok","timestamp":1578751007575,"user_tz":0,"elapsed":319792,"user":{"displayName":"Liam Butler","photoUrl":"","userId":"09361222935182582455"}}},"source":["pipeline.fit(x_train)"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('union',\n","                 FeatureUnion(n_jobs=None,\n","                              transformer_list=[('review',\n","                                                 Pipeline(memory=None,\n","                                                          steps=[('selector',\n","                                                                  ItemSelector(key='review')),\n","                                                                 ('tfidf',\n","                                                                  TfidfVectorizer(analyzer='word',\n","                                                                                  binary=False,\n","                                                                                  decode_error='strict',\n","                                                                                  dtype=<class 'numpy.float64'>,\n","                                                                                  encoding='utf-8',\n","                                                                                  input='content',\n","                                                                                  lowercase=True,\n","                                                                                  max_df=0.3,\n","                                                                                  max_features=1000,\n","                                                                                  min_df=3,\n","                                                                                  n...\n","                                                          verbose=False)),\n","                                                ('statistics',\n","                                                 Pipeline(memory=None,\n","                                                          steps=[('selector',\n","                                                                  ItemSelector(key=['polarity',\n","                                                                                    'subjectivity',\n","                                                                                    'length',\n","                                                                                    'full_length'])),\n","                                                                 ('statistics',\n","                                                                  TextStatistics()),\n","                                                                 ('vect',\n","                                                                  DictVectorizer(dtype=<class 'numpy.float64'>,\n","                                                                                 separator='=',\n","                                                                                 sort=True,\n","                                                                                 sparse=True))],\n","                                                          verbose=False))],\n","                              transformer_weights={'review': 1,\n","                                                   'statistics': 1},\n","                              verbose=False))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"Pojxzebiu6cm","colab_type":"code","colab":{}},"source":["# Create our set of Vocabulary   \n","vocabulary=get_vocabulary(x_train.review , 500)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MFmueFW3rtFM","colab_type":"code","colab":{}},"source":["train_vector1 = pipeline.transform(x_train)\n","test_vector1 = pipeline.transform(x_test)\n","val_vector1 = pipeline.transform(x_val)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IPrHdrKwFmG","colab_type":"code","colab":{}},"source":["train_vector2=[]\n","test_vector2=[]\n","val_vector2=[]\n","\n","# Create a vector for using our vocabulary\n","\n","for row in x_train.review:\n","    vector_instance=get_vector_text(vocabulary,row)\n","    train_vector2.append(vector_instance)\n","\n","for row in x_test.review:\n","    vector_instance=get_vector_text(vocabulary,row)\n","    test_vector2.append(vector_instance)\n","\n","for row in x_val.review:\n","    vector_instance=get_vector_text(vocabulary,row)\n","    val_vector2.append(vector_instance)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"62SQwHc5Ck7m","colab_type":"code","colab":{}},"source":["# Merge the two vectors \n","from scipy.sparse import csr_matrix, hstack\n","train_vector = hstack((csr_matrix(train_vector2),train_vector1))\n","test_vector = hstack((csr_matrix(test_vector2),test_vector1))\n","val_vector = hstack((csr_matrix(val_vector2),val_vector1))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5fvziPnvhZL","colab_type":"text"},"source":["Build the models"]},{"cell_type":"code","metadata":{"id":"Cu1puvSkp-H-","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","\n","# SVM classifier\n","svm_clf=SVC(gamma='auto') # Initialize the SVM model\n","\n","# Logistic regression\n","lr_clf = LogisticRegression()\n","\n","# Decision Tree Classifier\n","dt_clf = DecisionTreeClassifier()\n","\n","# naive bayes\n","nb_clf = GaussianNB()\n","\n","# random forest\n","rf_clf=RandomForestClassifier(n_estimators=100)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hLyNKADfqNzW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"9f9d7d88-2362-4c86-ecdc-7ca82b0d9b42","executionInfo":{"status":"ok","timestamp":1578752684944,"user_tz":0,"elapsed":1997090,"user":{"displayName":"Liam Butler","photoUrl":"","userId":"09361222935182582455"}}},"source":["import pandas as pd \n","from pandas import DataFrame\n","from sklearn.model_selection import cross_validate\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import precision_score\n","\n","# Set up scoring\n","lst = []\n","list_result =[]\n","cols = ['Model', 'Fold','Accuracy']\n","cols1 = ['Model', 'Accuracy', 'Recall', 'Precision', 'F1 - Score']\n","\n","clfs = [svm_clf, lr_clf, dt_clf, rf_clf]\n","cv = 5\n","scoring = {'acc': 'accuracy',\n","           'prec_macro': 'precision_macro',\n","           'rec_micro': 'recall_macro',\n","           'f1_score': 'f1'}\n","\n","# Run cross validation on build table of results\n","for clf in clfs:\n","    scores = cross_validate(clf,train_vector, y_train, cv=cv, scoring=scoring )\n","\n","    lst.append([str(clf), str(scores['test_acc'].mean()) ,str(scores['test_rec_micro'].mean()) ,str(scores['test_prec_macro'].mean()) ,str(scores['test_f1_score'].mean()) ]) \n","    \n","\n","    # Predict validation set\n","    clf.fit(train_vector, y_train )\n","    y_pred = clf.predict(val_vector)\n","    list_result.append((str(clf),accuracy_score(y_val, y_pred),recall_score(y_val, y_pred),precision_score(y_val, y_pred),f1_score(y_val, y_pred),))\n","\n","    print('Model ' + str(clf) + ' Complete' )\n"],"execution_count":61,"outputs":[{"output_type":"stream","text":["Model SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n","    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n","    max_iter=-1, probability=False, random_state=None, shrinking=True,\n","    tol=0.001, verbose=False) Complete\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["Model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False) Complete\n","Model DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n","                       max_depth=None, max_features=None, max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, presort='deprecated',\n","                       random_state=None, splitter='best') Complete\n","Model RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                       criterion='gini', max_depth=None, max_features='auto',\n","                       max_leaf_nodes=None, max_samples=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, n_estimators=100,\n","                       n_jobs=None, oob_score=False, random_state=None,\n","                       verbose=0, warm_start=False) Complete\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OXaJSrJjKM6-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":180},"outputId":"9213f901-6eda-40c8-8c95-4e9a8b88e41d","executionInfo":{"status":"ok","timestamp":1578752684948,"user_tz":0,"elapsed":1997074,"user":{"displayName":"Liam Butler","photoUrl":"","userId":"09361222935182582455"}}},"source":["print('Cross validation results')\n","dfResults1 = pd.DataFrame(lst, columns = cols1)\n","dfResults1\n"],"execution_count":62,"outputs":[{"output_type":"stream","text":["Cross validation results\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Accuracy</th>\n","      <th>Recall</th>\n","      <th>Precision</th>\n","      <th>F1 - Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>0.8065591691658336</td>\n","      <td>0.8066768655609534</td>\n","      <td>0.8100223432109704</td>\n","      <td>0.8158639689087099</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n","      <td>0.8526859713428857</td>\n","      <td>0.8527017579402856</td>\n","      <td>0.8527844024625564</td>\n","      <td>0.8534745750596084</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n","      <td>0.7304359880039988</td>\n","      <td>0.7304259196817384</td>\n","      <td>0.7305156313121085</td>\n","      <td>0.7287458056315205</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n","      <td>0.8341552815728092</td>\n","      <td>0.8341593076919583</td>\n","      <td>0.8343800787974057</td>\n","      <td>0.8341437060276086</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               Model  ...          F1 - Score\n","0  SVC(C=1.0, break_ties=False, cache_size=200, c...  ...  0.8158639689087099\n","1  LogisticRegression(C=1.0, class_weight=None, d...  ...  0.8534745750596084\n","2  DecisionTreeClassifier(ccp_alpha=0.0, class_we...  ...  0.7287458056315205\n","3  RandomForestClassifier(bootstrap=True, ccp_alp...  ...  0.8341437060276086\n","\n","[4 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"HGUKpb6ZYomS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":180},"outputId":"7912f6bf-689d-4253-d995-b7362c3b97f3","executionInfo":{"status":"ok","timestamp":1578752684949,"user_tz":0,"elapsed":1997061,"user":{"displayName":"Liam Butler","photoUrl":"","userId":"09361222935182582455"}}},"source":["print('Predict Validation set - Scores')\n","dfResultsValPredict = pd.DataFrame(list_result, columns = cols1)\n","dfResultsValPredict"],"execution_count":63,"outputs":[{"output_type":"stream","text":["Predict Validation set - Scores\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Accuracy</th>\n","      <th>Recall</th>\n","      <th>Precision</th>\n","      <th>F1 - Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>0.800880</td>\n","      <td>0.847956</td>\n","      <td>0.777010</td>\n","      <td>0.810934</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n","      <td>0.845462</td>\n","      <td>0.857086</td>\n","      <td>0.839425</td>\n","      <td>0.848163</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n","      <td>0.733507</td>\n","      <td>0.732037</td>\n","      <td>0.737010</td>\n","      <td>0.734515</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n","      <td>0.833467</td>\n","      <td>0.836443</td>\n","      <td>0.833465</td>\n","      <td>0.834951</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               Model  ...  F1 - Score\n","0  SVC(C=1.0, break_ties=False, cache_size=200, c...  ...    0.810934\n","1  LogisticRegression(C=1.0, class_weight=None, d...  ...    0.848163\n","2  DecisionTreeClassifier(ccp_alpha=0.0, class_we...  ...    0.734515\n","3  RandomForestClassifier(bootstrap=True, ccp_alp...  ...    0.834951\n","\n","[4 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"jSNtkhm0-l8Z","colab_type":"text"},"source":["Logistic regression is the best model so we can validate this against our test results"]},{"cell_type":"code","metadata":{"id":"6WTsXxEw5UlU","colab_type":"code","colab":{}},"source":["# Final model validation\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","list_result =[]\n","\n","\n","\n","lr_clf.fit(train_vector, y_train )\n","y_pred = lr_clf.predict(val_vector)\n","list_result.append((\"Logistic Regression\",accuracy_score(y_test, y_pred)))\n","\n","array = confusion_matrix(y_test, y_pred)\n","\n","print(list_result)\n"],"execution_count":0,"outputs":[]}]}